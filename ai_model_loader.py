# AI ëª¨ë¸ ë¡œë” ë° ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ
import os
import pickle
import joblib
import torch
import numpy as np
from typing import Dict, Any, List, Tuple
import logging
import warnings

warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AIModelManager:
    """í•™ìŠµëœ AI ëª¨ë¸ë“¤ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, models_dir: str = "models"):
        self.models_dir = models_dir
        self.emotion_model = None
        self.korean_bert_emotion = None
        self.emotion_pipeline = None
        self.loaded_models = {}

        # ëª¨ë¸ ë¡œë“œ
        self._load_models()

    def _load_models(self):
        """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì„ ë¡œë“œ"""
        try:
            # ê°ì •ë¶„ì„ ëª¨ë¸ ë¡œë“œ
            self._load_emotion_models()
            logger.info("âœ… AI ëª¨ë¸ë“¤ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _load_emotion_models(self):
        """ê°ì •ë¶„ì„ ëª¨ë¸ë“¤ ë¡œë“œ"""
        model_files = [
            "korean_emotion_complete_pipeline.pkl",
            "simple_emotion_pipeline.pkl",
            "simple_emotion_pipeline.joblib",
            "korean_bert_emotion.pkl",
        ]

        for model_file in model_files:
            model_path = os.path.join(self.models_dir, model_file)
            if os.path.exists(model_path):
                try:
                    if model_file.endswith(".joblib"):
                        model = joblib.load(model_path)
                    else:
                        with open(model_path, "rb") as f:
                            model = pickle.load(f)

                    self.loaded_models[model_file] = model
                    logger.info(f"âœ… {model_file} ë¡œë“œ ì™„ë£Œ")

                except Exception as e:
                    logger.warning(f"âš ï¸ {model_file} ë¡œë“œ ì‹¤íŒ¨: {e}")

    def analyze_emotion(self, text: str) -> Dict[str, Any]:
        """ê°ì •ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì•™ìƒë¸” ê°ì •ë¶„ì„
            emotions = {}
            confidence_scores = {}

            # ê° ë¡œë“œëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì‹œë„
            for model_name, model in self.loaded_models.items():
                try:
                    if hasattr(model, "predict"):
                        # sklearn ìŠ¤íƒ€ì¼ ëª¨ë¸
                        prediction = model.predict([text])
                        emotions[model_name] = prediction[0] if prediction else "ì¤‘ì„±"

                        if hasattr(model, "predict_proba"):
                            proba = model.predict_proba([text])
                            confidence_scores[model_name] = float(np.max(proba))

                    elif hasattr(model, "predict_emotion"):
                        # ì»¤ìŠ¤í…€ ê°ì •ë¶„ì„ ëª¨ë¸
                        result = model.predict_emotion(text)
                        emotions[model_name] = result.get("emotion", "ì¤‘ì„±")
                        confidence_scores[model_name] = result.get("confidence", 0.5)

                except Exception as e:
                    logger.warning(f"ëª¨ë¸ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

            # ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ê°ì •ë¶„ì„ (fallback)
            rule_based_emotion = self._rule_based_emotion_analysis(text)
            emotions["rule_based"] = rule_based_emotion["emotion"]
            confidence_scores["rule_based"] = rule_based_emotion["confidence"]

            # ìµœì¢… ê°ì • ê²°ì • (ì•™ìƒë¸”)
            final_emotion = self._ensemble_emotion_decision(emotions, confidence_scores)

            return {
                "emotion": final_emotion,
                "all_predictions": emotions,
                "confidence_scores": confidence_scores,
                "text_length": len(text),
                "analysis_status": "success",
            }

        except Exception as e:
            logger.error(f"ê°ì •ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "emotion": "ì¤‘ì„±",
                "all_predictions": {},
                "confidence_scores": {},
                "text_length": len(text),
                "analysis_status": "error",
                "error": str(e),
            }

    def _rule_based_emotion_analysis(self, text: str) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ ê°ì •ë¶„ì„ (fallback ë°©ë²•)"""

        # ê°ì • í‚¤ì›Œë“œ ì‚¬ì „
        emotion_keywords = {
            "ê¸ì •": [
                "ì¢‹ë‹¤",
                "í–‰ë³µ",
                "ê¸°ì˜ë‹¤",
                "ë§Œì¡±",
                "í›Œë¥­",
                "ì™„ë²½",
                "ìµœê³ ",
                "ê°ì‚¬",
                "ì‚¬ë‘",
                "ì¦ê²ë‹¤",
            ],
            "ë¶€ì •": [
                "ë‚˜ì˜ë‹¤",
                "ìŠ¬í”„ë‹¤",
                "í™”ë‚˜ë‹¤",
                "ì§œì¦",
                "ì‹¤ë§",
                "ê±±ì •",
                "ë¶ˆì•ˆ",
                "ë¬´ì„œ",
                "í˜ë“¤ë‹¤",
                "ì–´ë µë‹¤",
            ],
            "ë†€ëŒ": ["ë†€ëë‹¤", "ì‹ ê¸°", "ì™€", "í—‰", "ì–´ë¨¸", "ì„¸ìƒì—", "ì •ë§", "ì§„ì§œ"],
            "ì¤‘ì„±": ["ê·¸ëƒ¥", "ë³´í†µ", "ì¼ë°˜ì ", "í‰ë²”", "ê·¸ë ‡ë‹¤", "ìŒ", "ë„¤", "ì•„ë‹ˆì˜¤"],
        }

        text_lower = text.lower()
        emotion_scores = {emotion: 0 for emotion in emotion_keywords.keys()}

        # í‚¤ì›Œë“œ ë§¤ì¹­
        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    emotion_scores[emotion] += 1

        # ìµœê³  ì ìˆ˜ ê°ì • ì„ íƒ
        max_emotion = max(emotion_scores, key=emotion_scores.get)
        max_score = emotion_scores[max_emotion]

        # ì‹ ë¢°ë„ ê³„ì‚°
        total_matches = sum(emotion_scores.values())
        confidence = max_score / max(total_matches, 1) if total_matches > 0 else 0.5

        return {
            "emotion": max_emotion if max_score > 0 else "ì¤‘ì„±",
            "confidence": confidence,
            "keyword_scores": emotion_scores,
        }

    def _ensemble_emotion_decision(
        self, emotions: Dict, confidence_scores: Dict
    ) -> str:
        """ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ê°ì • ê²°ì •"""

        if not emotions:
            return "ì¤‘ì„±"

        # ì‹ ë¢°ë„ ê°€ì¤‘ íˆ¬í‘œ
        emotion_votes = {}
        for model_name, emotion in emotions.items():
            confidence = confidence_scores.get(model_name, 0.5)
            if emotion not in emotion_votes:
                emotion_votes[emotion] = 0
            emotion_votes[emotion] += confidence

        # ìµœê³  ë“í‘œ ê°ì • ë°˜í™˜
        return max(emotion_votes, key=emotion_votes.get)

    def analyze_conversation_context(self, text: str) -> Dict[str, Any]:
        """ëŒ€í™” ë§¥ë½ ë¶„ì„"""
        try:
            # í…ìŠ¤íŠ¸ ê¸°ë³¸ ë¶„ì„
            analysis = {
                "text_length": len(text),
                "word_count": len(text.split()),
                "has_question": "?" in text
                or any(
                    q in text
                    for q in ["ë­", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””", "ëˆ„êµ¬"]
                ),
                "has_exclamation": "!" in text,
                "politeness_level": self._analyze_politeness(text),
                "topic_keywords": self._extract_topic_keywords(text),
                "urgency_level": self._analyze_urgency(text),
                "conversation_type": self._classify_conversation_type(text),
            }

            return analysis

        except Exception as e:
            logger.error(f"ëŒ€í™” ë§¥ë½ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def _analyze_politeness(self, text: str) -> str:
        """ì •ì¤‘í•¨ ìˆ˜ì¤€ ë¶„ì„"""
        polite_markers = ["ìš”", "ìŠµë‹ˆë‹¤", "ì„¸ìš”", "í•´ì£¼ì„¸ìš”", "ë¶€íƒ", "ì£„ì†¡", "ì‹¤ë¡€"]
        casual_markers = ["ì•¼", "ë„ˆ", "í•´", "í•˜ì§€ë§ˆ", "ë­ì•¼"]

        polite_count = sum(1 for marker in polite_markers if marker in text)
        casual_count = sum(1 for marker in casual_markers if marker in text)

        if polite_count > casual_count:
            return "ì •ì¤‘í•¨"
        elif casual_count > polite_count:
            return "ì¹œê·¼í•¨"
        else:
            return "ë³´í†µ"

    def _extract_topic_keywords(self, text: str) -> List[str]:
        """ì£¼ì œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        topic_categories = {
            "ê±´ê°•": ["ê±´ê°•", "ë³‘", "ì•„í”„ë‹¤", "ì¹˜ë£Œ", "ì˜ì‚¬", "ë³‘ì›", "ì•½", "ì¦ìƒ"],
            "ê¸ˆìœµ": ["ëˆ", "íˆ¬ì", "ì£¼ì‹", "ì€í–‰", "ëŒ€ì¶œ", "í€ë“œ", "ë³´í—˜", "ì ê¸ˆ"],
            "ë²•ë¥ ": ["ë²•", "ë³€í˜¸ì‚¬", "ì†Œì†¡", "ê³„ì•½", "ê¶Œë¦¬", "ì˜ë¬´", "ë²•ì›"],
            "ê¸°ìˆ ": [
                "ì»´í“¨í„°",
                "í”„ë¡œê·¸ë¨",
                "AI",
                "ì¸ê³µì§€ëŠ¥",
                "ê°œë°œ",
                "ì½”ë”©",
                "ì†Œí”„íŠ¸ì›¨ì–´",
            ],
            "êµìœ¡": ["ê³µë¶€", "í•™êµ", "ì‹œí—˜", "êµìœ¡", "ë°°ìš°ë‹¤", "ê°€ë¥´ì¹˜ë‹¤", "ìˆ˜ì—…"],
        }

        found_topics = []
        for topic, keywords in topic_categories.items():
            if any(keyword in text for keyword in keywords):
                found_topics.append(topic)

        return found_topics

    def _analyze_urgency(self, text: str) -> str:
        """ê¸´ê¸‰ë„ ë¶„ì„"""
        urgent_markers = ["ê¸‰", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "ë‹¹ì¥", "ì‘ê¸‰", "ê¸´ê¸‰", "ì‹¬ê°", "ìœ„í—˜"]

        if any(marker in text for marker in urgent_markers):
            return "ë†’ìŒ"
        elif "?" in text:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"

    def _classify_conversation_type(self, text: str) -> str:
        """ëŒ€í™” ìœ í˜• ë¶„ë¥˜"""
        if "?" in text:
            return "ì§ˆë¬¸"
        elif any(greeting in text for greeting in ["ì•ˆë…•", "hello", "hi"]):
            return "ì¸ì‚¬"
        elif any(thanks in text for thanks in ["ê°ì‚¬", "ê³ ë§ˆì›Œ", "thank"]):
            return "ê°ì‚¬"
        elif any(req in text for req in ["í•´ì£¼ì„¸ìš”", "ë¶€íƒ", "ë„ì™€ì£¼", "help"]):
            return "ìš”ì²­"
        else:
            return "ì¼ë°˜ëŒ€í™”"

    def generate_domain_specific_analysis(
        self, text: str, domain: str
    ) -> Dict[str, Any]:
        """ì „ë¬¸ ë¶„ì•¼ë³„ ì‹¤ì œ AI ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ê°ì •ë¶„ì„ê³¼ ë§¥ë½ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë„ë©”ì¸ë³„ ë¶„ì„
            emotion_analysis = self.analyze_emotion(text)
            context_analysis = self.analyze_conversation_context(text)

            if domain == "financial":
                return self._analyze_financial_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "medical":
                return self._analyze_medical_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "legal":
                return self._analyze_legal_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "tech":
                return self._analyze_technical_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "data":
                return self._analyze_data_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "creative":
                return self._analyze_creative_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "marketing":
                return self._analyze_marketing_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "education":
                return self._analyze_education_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "hr":
                return self._analyze_hr_query(text, emotion_analysis, context_analysis)
            elif domain == "sales":
                return self._analyze_sales_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "research":
                return self._analyze_research_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "translation":
                return self._analyze_translation_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "consulting":
                return self._analyze_consulting_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "psychology":
                return self._analyze_psychology_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "startup":
                return self._analyze_startup_query(
                    text, emotion_analysis, context_analysis
                )
            elif domain == "wellness":
                return self._analyze_wellness_query(
                    text, emotion_analysis, context_analysis
                )
            else:
                return self._analyze_general_query(
                    text, emotion_analysis, context_analysis
                )

        except Exception as e:
            logger.error(f"ë„ë©”ì¸ë³„ ë¶„ì„ ì˜¤ë¥˜ ({domain}): {e}")
            return {"error": str(e), "analysis_type": "fallback"}

    def _analyze_financial_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ê¸ˆìœµ ë¶„ì•¼ ì „ë¬¸ ë¶„ì„"""
        text_lower = text.lower()

        # ê¸ˆìœµ ìš©ì–´ ë¶„ì„
        financial_terms = {
            "íˆ¬ì": ["íˆ¬ì", "investment", "í€ë“œ", "ì£¼ì‹", "ì±„ê¶Œ", "í¬íŠ¸í´ë¦¬ì˜¤"],
            "ë¦¬ìŠ¤í¬": ["ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ì†ì‹¤", "ë³€ë™ì„±", "ë¶ˆí™•ì‹¤ì„±"],
            "ìˆ˜ìµ": ["ìˆ˜ìµ", "ì´ìµ", "ë°°ë‹¹", "ê¸ˆë¦¬", "ìˆ˜ìµë¥ ", "ë³µë¦¬"],
            "ê²½ì œ": ["ê²½ì œ", "ê²½ê¸°", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "GDP", "ì¤‘ì•™ì€í–‰"],
            "ë³´í—˜": ["ë³´í—˜", "ì—°ê¸ˆ", "ì €ì¶•", "ì€í‡´", "ì—°ê¸ˆë³´í—˜"],
            "ëŒ€ì¶œ": ["ëŒ€ì¶œ", "ëª¨ê¸°ì§€", "ì‹ ìš©", "ë‹´ë³´", "ì´ì"],
        }

        # íŠ¹ì • ê¸ˆìœµ ìƒí’ˆ/ê°œë… ë¶„ì„
        specific_concepts = {
            "nps": {"name": "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨", "type": "ì—°ê¸ˆì œë„", "category": "ë…¸í›„ì¤€ë¹„"},
            "isa": {
                "name": "ê°œì¸ì¢…í•©ìì‚°ê´€ë¦¬ê³„ì¢Œ",
                "type": "ì„¸ì œí˜œíƒê³„ì¢Œ",
                "category": "ì ˆì„¸íˆ¬ì",
            },
            "etf": {"name": "ìƒì¥ì§€ìˆ˜í€ë“œ", "type": "íˆ¬ììƒí’ˆ", "category": "ê°„ì ‘íˆ¬ì"},
            "reit": {
                "name": "ë¶€ë™ì‚°íˆ¬ìì‹ íƒ",
                "type": "íˆ¬ììƒí’ˆ",
                "category": "ë¶€ë™ì‚°íˆ¬ì",
            },
        }

        # í‚¤ì›Œë“œ ë§¤ì¹­
        detected_categories = []
        detected_concepts = []

        for category, keywords in financial_terms.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_categories.append(category)

        for concept, info in specific_concepts.items():
            if concept in text_lower:
                detected_concepts.append({concept: info})

        # ê¸´ê¸‰ë„ ë° ë³µì¡ë„ í‰ê°€
        urgency = context.get("urgency_level", "ë‚®ìŒ")
        complexity = (
            "ë†’ìŒ"
            if len(detected_categories) > 2
            else "ë³´í†µ" if detected_categories else "ë‚®ìŒ"
        )

        # ê°ì • ê¸°ë°˜ í†¤ ì¡°ì ˆ
        emotional_state = emotion.get("emotion", "ì¤‘ì„±")
        if emotional_state == "ë¶€ì •":
            tone = "ì•ˆì‹¬ì‹œí‚¤ëŠ”"
        elif emotional_state == "ê¸ì •":
            tone = "ê²©ë ¤í•˜ëŠ”"
        else:
            tone = "ì „ë¬¸ì ì¸"

        return {
            "analysis_type": "financial_expert",
            "detected_categories": detected_categories,
            "detected_concepts": detected_concepts,
            "complexity_level": complexity,
            "urgency_level": urgency,
            "emotional_tone": tone,
            "specific_analysis": self._generate_financial_insights(
                text_lower, detected_categories, detected_concepts
            ),
            "recommendation_type": self._determine_financial_recommendation_type(
                detected_categories, urgency
            ),
        }

    def _generate_financial_insights(
        self, text: str, categories: list, concepts: list
    ) -> Dict[str, Any]:
        """ê¸ˆìœµ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights: Dict[str, Any] = {
            "market_context": "í˜„ì¬ ì‹œì¥ ìƒí™© ê³ ë ¤ í•„ìš”",
            "risk_assessment": "ì¤‘ê°„ ìˆ˜ì¤€",
            "time_horizon": "ì¤‘ì¥ê¸° ê´€ì  ê¶Œì¥",
        }

        # NPS íŠ¹ë³„ ë¶„ì„
        if any("nps" in str(concept).lower() for concept in concepts):
            nps_analysis = {
                "ì •ì˜": "êµ­ë¯¼ì—°ê¸ˆ (National Pension Service)",
                "íŠ¹ì§•": "êµ­ê°€ì—ì„œ ìš´ì˜í•˜ëŠ” ê³µì ì—°ê¸ˆì œë„",
                "ì¥ì ": ["ê°•ì œê°€ì…ìœ¼ë¡œ ë…¸í›„ë³´ì¥", "ì¸í”Œë ˆì´ì…˜ ì—°ë™", "ìœ ì¡±ê¸‰ì—¬ í¬í•¨"],
                "ë‹¨ì ": ["ìˆ˜ìµë¥  ì œí•œì ", "ì •ì¹˜ì  ë¦¬ìŠ¤í¬", "ê°œì¸ì„ íƒê¶Œ ë¶€ì¡±"],
                "ì „ëµ": "ê°œì¸ì—°ê¸ˆ(IRP, ì—°ê¸ˆì €ì¶•)ê³¼ í•¨ê»˜ 3ì¸µ ì—°ê¸ˆì²´ê³„ êµ¬ì¶• ê¶Œì¥",
            }
            insights["nps_analysis"] = nps_analysis

        if "íˆ¬ì" in categories:
            insights["investment_strategy"] = "ë¶„ì‚°íˆ¬ì ë° ì¥ê¸°íˆ¬ì ì›ì¹™ ì ìš©"

        if "ë¦¬ìŠ¤í¬" in categories:
            insights["risk_management"] = "ì ì ˆí•œ ìì‚°ë°°ë¶„ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ê´€ë¦¬"

        return insights

    def _determine_financial_recommendation_type(
        self, categories: list, urgency: str
    ) -> str:
        """ê¸ˆìœµ ì¶”ì²œ ìœ í˜• ê²°ì •"""
        if urgency == "ë†’ìŒ":
            return "ì¦‰ì‹œ_ì¡°ì¹˜_í•„ìš”"
        elif "íˆ¬ì" in categories and "ë¦¬ìŠ¤í¬" in categories:
            return "í¬íŠ¸í´ë¦¬ì˜¤_ê²€í† "
        elif "ë³´í—˜" in categories or "ì—°ê¸ˆ" in categories:
            return "ì¥ê¸°_ê³„íš_ìˆ˜ë¦½"
        else:
            return "ì¼ë°˜_ìƒë‹´"

    def _analyze_data_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ë°ì´í„° ë¶„ì•¼ ì „ë¬¸ ë¶„ì„"""
        text_lower = text.lower()

        # ë°ì´í„° ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„
        data_terms = {
            "í†µê³„": ["í†µê³„", "statistics", "í‰ê· ", "ë¶„ì‚°", "í‘œì¤€í¸ì°¨", "í™•ë¥ "],
            "ë¶„ì„": ["ë¶„ì„", "analysis", "ë°ì´í„°ë§ˆì´ë‹", "íŒ¨í„´", "ì¸ì‚¬ì´íŠ¸"],
            "ì‹œê°í™”": ["ì‹œê°í™”", "visualization", "ì°¨íŠ¸", "ê·¸ë˜í”„", "í”Œë¡¯"],
            "ë¨¸ì‹ ëŸ¬ë‹": ["ë¨¸ì‹ ëŸ¬ë‹", "machine learning", "ai", "ë”¥ëŸ¬ë‹", "ëª¨ë¸"],
            "ë¹…ë°ì´í„°": ["ë¹…ë°ì´í„°", "big data", "í•˜ë‘¡", "ìŠ¤íŒŒí¬", "ë¶„ì‚°ì²˜ë¦¬"],
        }

        # í‚¤ì›Œë“œ ë§¤ì¹­
        detected_categories = []
        for category, keywords in data_terms.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_categories.append(category)

        # ë³µì¡ë„ ë° ë¶„ì„ ìˆ˜ì¤€ ê²°ì •
        if len(detected_categories) > 2:
            complexity = "ê³ ê¸‰"
        elif detected_categories:
            complexity = "ì¤‘ê¸‰"
        else:
            complexity = "ê¸°ì´ˆ"

        return {
            "analysis_type": "data_expert",
            "detected_categories": detected_categories,
            "complexity_level": complexity,
            "specific_analysis": self._generate_data_insights(
                text_lower, detected_categories
            ),
            "recommendation_type": "ì „ë¬¸_ë¶„ì„",
        }

    def _generate_data_insights(self, text: str, categories: list) -> Dict[str, Any]:
        """ë°ì´í„° ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights: Dict[str, Any] = {
            "methodology": "ê³¼í•™ì  ì ‘ê·¼ë²•",
            "tools": "ì „ë¬¸ ë„êµ¬ í™œìš©",
            "application": "ì‹¤ë¬´ ì ìš©",
        }

        if "í†µê³„" in categories:
            insights["statistics"] = {
                "ê¸°ì´ˆ": "ê¸°ìˆ í†µê³„, í™•ë¥ ë¶„í¬, ê°€ì„¤ê²€ì •",
                "ì‘ìš©": "íšŒê·€ë¶„ì„, ì‹œê³„ì—´ ë¶„ì„, ë² ì´ì§€ì•ˆ í†µê³„",
                "í™œìš©": "ë°ì´í„° ê²€ì¦, íŒ¨í„´ ë°œê²¬, ì˜ˆì¸¡ ëª¨ë¸ë§",
            }

        if "ë¨¸ì‹ ëŸ¬ë‹" in categories:
            insights["ml"] = {
                "ì§€ë„í•™ìŠµ": "ë¶„ë¥˜, íšŒê·€ ì•Œê³ ë¦¬ì¦˜",
                "ë¹„ì§€ë„í•™ìŠµ": "í´ëŸ¬ìŠ¤í„°ë§, ì°¨ì›ì¶•ì†Œ",
                "ë”¥ëŸ¬ë‹": "ì‹ ê²½ë§, CNN, RNN",
            }

        return insights

    def _analyze_medical_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ì˜ë£Œ ë¶„ì•¼ ì „ë¬¸ ë¶„ì„"""
        # ì˜ë£Œ ë¶„ì•¼ ë¶„ì„ ë¡œì§ êµ¬í˜„
        return {
            "analysis_type": "medical_expert",
            "urgency": context.get("urgency_level", "ë³´í†µ"),
            "emotional_support_needed": emotion.get("emotion") == "ë¶€ì •",
        }

    def _analyze_legal_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ë²•ë¥  ë¶„ì•¼ ì „ë¬¸ ë¶„ì„"""
        # ë²•ë¥  ë¶„ì•¼ ë¶„ì„ ë¡œì§ êµ¬í˜„
        return {
            "analysis_type": "legal_expert",
            "complexity": "ë†’ìŒ" if "ê³„ì•½" in text or "ì†Œì†¡" in text else "ë³´í†µ",
        }

    def _analyze_technical_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ê¸°ìˆ  ë¶„ì•¼ ì „ë¬¸ ë¶„ì„"""
        # ê¸°ìˆ  ë¶„ì•¼ ë¶„ì„ ë¡œì§ êµ¬í˜„
        return {
            "analysis_type": "technical_expert",
            "tech_level": (
                "ê³ ê¸‰"
                if any(term in text for term in ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹"])
                else "ê¸°ë³¸"
            ),
        }

    def _analyze_general_query(
        self, text: str, emotion: Dict, context: Dict
    ) -> Dict[str, Any]:
        """ì¼ë°˜ ë¶„ì•¼ ë¶„ì„"""
        return {
            "analysis_type": "general_expert",
            "topic_analysis": context.get("topic_keywords", []),
        }

    def generate_smart_response(self, text: str, agent_type: str = "general") -> str:
        """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„±"""
        try:
            # ë„ë©”ì¸ë³„ ë¶„ì„ ìˆ˜í–‰
            domain_analysis = self.generate_domain_specific_analysis(text, agent_type)

            # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°
            if domain_analysis.get("analysis_type") == "financial_expert":
                return self._format_financial_response(text, domain_analysis)
            elif domain_analysis.get("analysis_type") == "medical_expert":
                return self._format_medical_response(text, domain_analysis)
            elif domain_analysis.get("analysis_type") == "legal_expert":
                return self._format_legal_response(text, domain_analysis)
            elif domain_analysis.get("analysis_type") == "data_expert":
                return self._format_data_response(text, domain_analysis)
            else:
                return self._format_general_response(text, domain_analysis)

        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _format_financial_response(self, query: str, analysis: Dict[str, Any]) -> str:
        """ê¸ˆìœµ ë¶„ì•¼ ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []

        # í—¤ë”
        tone = analysis.get("emotional_tone", "ì „ë¬¸ì ì¸")
        response_parts.append(f"ğŸ’° **ê²½ì œí•™ë°•ì‚¬ê¸‰ {tone} ë¶„ì„**")
        response_parts.append(f"ë¬¸ì˜: '{query}'")

        # NPS íŠ¹ë³„ ë¶„ì„
        if "nps_analysis" in analysis.get("specific_analysis", {}):
            nps_info = analysis["specific_analysis"]["nps_analysis"]
            response_parts.append("\nğŸ“Š **NPS (êµ­ë¯¼ì—°ê¸ˆ) ì „ë¬¸ ë¶„ì„**")
            response_parts.append(f"â€¢ **ì •ì˜**: {nps_info['ì •ì˜']}")
            response_parts.append(f"â€¢ **íŠ¹ì§•**: {nps_info['íŠ¹ì§•']}")
            response_parts.append("â€¢ **ì¥ì **:")
            for advantage in nps_info["ì¥ì "]:
                response_parts.append(f"  - {advantage}")
            response_parts.append("â€¢ **í•œê³„ì **:")
            for disadvantage in nps_info["ë‹¨ì "]:
                response_parts.append(f"  - {disadvantage}")
            response_parts.append(f"â€¢ **ì „ëµì  ì œì•ˆ**: {nps_info['ì „ëµ']}")

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        categories = analysis.get("detected_categories", [])
        if categories:
            response_parts.append(f"\nğŸ¯ **ê°ì§€ëœ ê´€ì‹¬ ë¶„ì•¼**: {', '.join(categories)}")

        # ë³µì¡ë„ ë° ì¶”ì²œ
        complexity = analysis.get("complexity_level", "ë³´í†µ")
        response_parts.append(f"\nğŸ“ˆ **ë¶„ì„ ë³µì¡ë„**: {complexity}")

        recommendation = analysis.get("recommendation_type", "ì¼ë°˜_ìƒë‹´")
        if recommendation == "ì¦‰ì‹œ_ì¡°ì¹˜_í•„ìš”":
            response_parts.append("âš ï¸ **ì¦‰ì‹œ ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤**")
        elif recommendation == "í¬íŠ¸í´ë¦¬ì˜¤_ê²€í† ":
            response_parts.append("ğŸ“‹ **í¬íŠ¸í´ë¦¬ì˜¤ ì „ì²´ì  ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤**")
        elif recommendation == "ì¥ê¸°_ê³„íš_ìˆ˜ë¦½":
            response_parts.append("ğŸ“… **ì¥ê¸°ì  ì¬ì • ê³„íš ìˆ˜ë¦½ì„ ê¶Œì¥í•©ë‹ˆë‹¤**")

        return "\n".join(response_parts)

    def _format_medical_response(self, query: str, analysis: Dict[str, Any]) -> str:
        """ì˜ë£Œ ë¶„ì•¼ ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []

        # í—¤ë”
        response_parts.append(f"ğŸ¥ **ì˜í•™ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„**")
        response_parts.append(f"ë¬¸ì˜: '{query}'")

        # ê¸°ë³¸ ì˜ë£Œ ë¶„ì„
        urgency = analysis.get("urgency", "ë³´í†µ")
        if urgency == "ë†’ìŒ":
            response_parts.append(
                "\nâš ï¸ **ì‘ê¸‰ ìƒí™© ê°ì§€** - ì¦‰ì‹œ ì˜ë£Œê¸°ê´€ ë°©ë¬¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            )

        response_parts.append("\nğŸ“‹ **ì˜ë£Œ ì „ë¬¸ê°€ ë¶„ì„**:")
        response_parts.append("â€¢ ì¦ìƒ í‰ê°€ ë° ì›ì¸ ë¶„ì„")
        response_parts.append("â€¢ ì ì ˆí•œ ì§„ë£Œê³¼ ì•ˆë‚´")
        response_parts.append("â€¢ ì˜ˆë°© ë° ê´€ë¦¬ ë°©ë²• ì œì‹œ")

        return "\n".join(response_parts)

    def _format_legal_response(self, query: str, analysis: Dict[str, Any]) -> str:
        """ë²•ë¥  ë¶„ì•¼ ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []

        response_parts.append(f"âš–ï¸ **ë²•í•™ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„**")
        response_parts.append(f"ë¬¸ì˜: '{query}'")

        response_parts.append("\nğŸ“– **ë²•ë¥  ì „ë¬¸ê°€ ë¶„ì„**:")
        response_parts.append("â€¢ ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ê²€í† ")
        response_parts.append("â€¢ ë²•ì  ìŸì  ë° í•´ê²°ë°©ì•ˆ")
        response_parts.append("â€¢ ì „ë¬¸ê°€ ìƒë‹´ í•„ìš”ì„± í‰ê°€")

        return "\n".join(response_parts)

    def _format_data_response(self, query: str, analysis: Dict[str, Any]) -> str:
        """ë°ì´í„° ë¶„ì•¼ ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []

        # í—¤ë”
        response_parts.append(f"ğŸ“Š **ë°ì´í„°ê³¼í•™ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„**")
        response_parts.append(f"ë¬¸ì˜: '{query}'")

        # ê°ì§€ëœ ì¹´í…Œê³ ë¦¬
        categories = analysis.get("detected_categories", [])
        if categories:
            response_parts.append(f"\nğŸ¯ **ë¶„ì„ ì˜ì—­**: {', '.join(categories)}")

        # ë³µì¡ë„ í‘œì‹œ
        complexity = analysis.get("complexity_level", "ê¸°ì´ˆ")
        response_parts.append(f"ğŸ“ˆ **ë¶„ì„ ìˆ˜ì¤€**: {complexity}")

        # êµ¬ì²´ì  ë¶„ì„ ë‚´ìš©
        specific_analysis = analysis.get("specific_analysis", {})

        # í†µê³„í•™ íŠ¹ë³„ ë¶„ì„
        if "í†µê³„" in categories and "statistics" in specific_analysis:
            stats_info = specific_analysis["statistics"]
            response_parts.append("\nğŸ“Š **í†µê³„í•™ ì „ë¬¸ ë¶„ì„**:")
            response_parts.append(f"â€¢ **ê¸°ì´ˆ ê°œë…**: {stats_info['ê¸°ì´ˆ']}")
            response_parts.append(f"â€¢ **ì‘ìš© ë¶„ì•¼**: {stats_info['ì‘ìš©']}")
            response_parts.append(f"â€¢ **ì‹¤ë¬´ í™œìš©**: {stats_info['í™œìš©']}")

        # ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„
        if "ë¨¸ì‹ ëŸ¬ë‹" in categories and "ml" in specific_analysis:
            ml_info = specific_analysis["ml"]
            response_parts.append("\nğŸ¤– **ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„**:")
            response_parts.append(f"â€¢ **ì§€ë„í•™ìŠµ**: {ml_info['ì§€ë„í•™ìŠµ']}")
            response_parts.append(f"â€¢ **ë¹„ì§€ë„í•™ìŠµ**: {ml_info['ë¹„ì§€ë„í•™ìŠµ']}")
            response_parts.append(f"â€¢ **ë”¥ëŸ¬ë‹**: {ml_info['ë”¥ëŸ¬ë‹']}")

        # ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ì¡°ì–¸
        if not categories or len(categories) == 0:
            response_parts.append("\nğŸ’¡ **ë°ì´í„° ë¶„ì„ ê¸°ë³¸ ì ‘ê·¼ë²•**:")
            response_parts.append("â€¢ **ë°ì´í„° ìˆ˜ì§‘**: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í™•ë³´")
            response_parts.append("â€¢ **ì „ì²˜ë¦¬**: ë°ì´í„° ì •ì œ ë° ë³€í™˜")
            response_parts.append("â€¢ **íƒìƒ‰ì  ë¶„ì„**: íŒ¨í„´ ë° íŠ¸ë Œë“œ ë°œê²¬")
            response_parts.append("â€¢ **ëª¨ë¸ë§**: ì ì ˆí•œ ë¶„ì„ ê¸°ë²• ì ìš©")
            response_parts.append("â€¢ **ê²€ì¦**: ê²°ê³¼ì˜ íƒ€ë‹¹ì„± í™•ì¸")

        response_parts.append(
            "\nğŸ“ **ì¶”ì²œ í•™ìŠµ ê²½ë¡œ**: ê¸°ì´ˆ í†µê³„ â†’ í”„ë¡œê·¸ë˜ë° â†’ ë¨¸ì‹ ëŸ¬ë‹ â†’ ì‹¤ë¬´ í”„ë¡œì íŠ¸"
        )

        return "\n".join(response_parts)

    def _format_general_response(self, query: str, analysis: Dict[str, Any]) -> str:
        """ì¼ë°˜ ë¶„ì•¼ ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []

        # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ì´ëª¨ì§€ ì„ íƒ
        analysis_type = analysis.get("analysis_type", "general_expert")
        if "data" in analysis_type.lower():
            emoji = "ğŸ“Š"
            title = "ë°ì´í„°ê³¼í•™ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„"
        elif "education" in analysis_type.lower():
            emoji = "ğŸ“š"
            title = "êµìœ¡í•™ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„"
        else:
            emoji = "ğŸ“"
            title = "ë°•ì‚¬ê¸‰ ì „ë¬¸ ë¶„ì„"

        response_parts.append(f"{emoji} **{title}**")
        response_parts.append(f"ë¬¸ì˜: '{query}'")

        # í† í”½ ë¶„ì„ ê²°ê³¼
        topic_keywords = analysis.get("topic_analysis", [])
        if topic_keywords:
            response_parts.append(f"\nğŸ¯ **ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(topic_keywords)}")

        # ê¸°ë³¸ ì „ë¬¸ê°€ ë¶„ì„
        response_parts.append("\nğŸ’¡ **ì „ë¬¸ê°€ ë¶„ì„**:")

        # í†µê³„í•™ ê´€ë ¨ íŠ¹ë³„ ì²˜ë¦¬
        if "í†µê³„" in query.lower() or "statistics" in query.lower():
            response_parts.append("â€¢ **í†µê³„í•™ ê°œë…**: ë°ì´í„° ìˆ˜ì§‘, ë¶„ì„, í•´ì„ì˜ ê³¼í•™")
            response_parts.append(
                "â€¢ **ì£¼ìš” ë¶„ì•¼**: ê¸°ìˆ í†µê³„í•™, ì¶”ë¡ í†µê³„í•™, ë² ì´ì§€ì•ˆ í†µê³„"
            )
            response_parts.append("â€¢ **ì‹¤ë¬´ ì ìš©**: ê°€ì„¤ê²€ì •, íšŒê·€ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹")
            response_parts.append("â€¢ **ìµœì‹  ë™í–¥**: ë¹…ë°ì´í„°, AI, ì˜ˆì¸¡ëª¨ë¸ë§")
        else:
            response_parts.append("â€¢ ì „ë¬¸ì  ì§€ì‹ ê¸°ë°˜ ë¶„ì„")
            response_parts.append("â€¢ ì‹¤ë¬´ ì ìš© ë°©ì•ˆ ì œì‹œ")
            response_parts.append("â€¢ ì¶”ê°€ í•™ìŠµ ìë£Œ ì•ˆë‚´")

        return "\n".join(response_parts)
        """AI ëª¨ë¸ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„±"""
        try:
            # ê°ì • ë¶„ì„
            emotion_analysis = self.analyze_emotion(text)

            # ëŒ€í™” ë§¥ë½ ë¶„ì„
            context_analysis = self.analyze_conversation_context(text)

            # ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            response = self._generate_contextual_response(
                text, emotion_analysis, context_analysis, agent_type
            )

            return response

        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def _generate_contextual_response(
        self, text: str, emotion: Dict, context: Dict, agent_type: str
    ) -> str:
        """ë§¥ë½ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""

        # ê°ì •ì— ë”°ë¥¸ ì‘ë‹µ í†¤ ì¡°ì ˆ
        emotion_type = emotion.get("emotion", "ì¤‘ì„±")
        conversation_type = context.get("conversation_type", "ì¼ë°˜ëŒ€í™”")
        urgency = context.get("urgency_level", "ë‚®ìŒ")

        # ì‘ë‹µ í”„ë¦¬í”½ìŠ¤ ê²°ì •
        if emotion_type == "ë¶€ì •":
            if urgency == "ë†’ìŒ":
                prefix = "ìƒí™©ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤. ì‹ ì†í•˜ê²Œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. "
            else:
                prefix = "ê±±ì •ì´ ë§ìœ¼ì‹œêµ°ìš”. ì°¨ê·¼ì°¨ê·¼ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤. "
        elif emotion_type == "ê¸ì •":
            prefix = "ê¸ì •ì ì¸ ë§ˆìŒê°€ì§ì´ ëŠê»´ì§‘ë‹ˆë‹¤! "
        elif conversation_type == "ì§ˆë¬¸":
            prefix = "ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤. "
        else:
            prefix = ""

        # ì—ì´ì „íŠ¸ íƒ€ì…ë³„ ì „ë¬¸ì„± ì¶”ê°€
        agent_expertise = {
            "medical": "ì˜í•™ì  ê´€ì ì—ì„œ ",
            "financial": "ê¸ˆìœµ ì „ë¬¸ê°€ë¡œì„œ ",
            "legal": "ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ",
            "technical": "ê¸°ìˆ  ì „ë¬¸ê°€ë¡œì„œ ",
        }

        expertise_note = agent_expertise.get(agent_type, "")

        return f"{prefix}{expertise_note}ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."


# ì „ì—­ AI ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
ai_manager = None


def get_ai_manager() -> AIModelManager:
    """AI ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global ai_manager
    if ai_manager is None:
        ai_manager = AIModelManager()
    return ai_manager


def analyze_text_with_ai(text: str, agent_type: str = "general") -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ë¥¼ AIë¡œ ë¶„ì„í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    manager = get_ai_manager()

    emotion_analysis = manager.analyze_emotion(text)
    context_analysis = manager.analyze_conversation_context(text)
    smart_response = manager.generate_smart_response(text, agent_type)

    return {
        "emotion_analysis": emotion_analysis,
        "context_analysis": context_analysis,
        "smart_response": smart_response,
        "ai_status": "active",
    }
