#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„ - ê³ í’ˆì§ˆ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì „ë¬¸ê°€
Advanced Data Analytics AI with Professional Analysis Capabilities
"""

import sqlite3
import json
import datetime
import random
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass


@dataclass
class AnalysisProject:
    """ë¶„ì„ í”„ë¡œì íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""

    id: int
    title: str
    data_source: str
    analysis_type: str
    status: str
    insights: str
    created_at: str


class DataAnalyticsGoblin:
    """ğŸ“Š ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„ - ê³ í’ˆì§ˆ ë°ì´í„° ì „ë¬¸ê°€"""

    def __init__(self, workspace_dir="./analytics_workspace"):
        self.name = "ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„"
        self.emoji = "ğŸ“Š"
        self.description = "ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ì „ë¬¸ê°€"

        # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
        self.workspace_dir = Path(workspace_dir)
        self.workspace_dir.mkdir(exist_ok=True)

        # ë¶„ì„ ì „ë¬¸ ë””ë ‰í† ë¦¬
        for subdir in [
            "datasets",
            "reports",
            "visualizations",
            "models",
            "exports",
            "dashboards",
        ]:
            (self.workspace_dir / subdir).mkdir(exist_ok=True)

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db_path = self.workspace_dir / "analytics_projects.db"
        self.init_database()

        # ë¡œê¹… ì„¤ì •
        log_file = self.workspace_dir / "analytics.log"
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_file, encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

        # ë¶„ì„ ì „ë¬¸ ê¸°ëŠ¥
        self.analysis_types = [
            "ê¸°ìˆ í†µê³„",
            "ìƒê´€ë¶„ì„",
            "íšŒê·€ë¶„ì„",
            "ë¶„ë¥˜ë¶„ì„",
            "ì‹œê³„ì—´ë¶„ì„",
            "í´ëŸ¬ìŠ¤í„°ë§",
        ]
        self.chart_types = [
            "ë§‰ëŒ€ê·¸ë˜í”„",
            "ì„ ê·¸ë˜í”„",
            "ì‚°ì ë„",
            "íˆíŠ¸ë§µ",
            "ë°•ìŠ¤í”Œë¡¯",
            "íˆìŠ¤í† ê·¸ë¨",
        ]
        self.metrics = [
            "í‰ê· ",
            "ì¤‘ì•™ê°’",
            "í‘œì¤€í¸ì°¨",
            "ìƒê´€ê³„ìˆ˜",
            "RÂ²",
            "RMSE",
            "ì •í™•ë„",
            "F1-Score",
        ]

        # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams["font.family"] = "DejaVu Sans"
        plt.rcParams["axes.unicode_minus"] = False

        self.logger.info(f"{self.name} ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"âœ… {self.emoji} {self.name} ë¶„ì„ ë© ì¤€ë¹„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì›Œí¬ìŠ¤í˜ì´ìŠ¤: {self.workspace_dir.absolute()}")

    def init_database(self):
        """ë¶„ì„ ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë¶„ì„ í”„ë¡œì íŠ¸ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS analysis_projects (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    description TEXT,
                    data_source TEXT NOT NULL,
                    data_type TEXT DEFAULT 'csv',
                    analysis_type TEXT NOT NULL,
                    target_variable TEXT,
                    features TEXT,
                    model_type TEXT,
                    status TEXT DEFAULT 'initiated',
                    insights TEXT,
                    recommendations TEXT,
                    accuracy_score REAL DEFAULT 0.0,
                    r_squared REAL DEFAULT 0.0,
                    data_quality_score REAL DEFAULT 0.0,
                    file_path TEXT,
                    visualization_path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ë°ì´í„°ì…‹ ì •ë³´ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS datasets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    description TEXT,
                    file_path TEXT NOT NULL,
                    file_size INTEGER,
                    row_count INTEGER,
                    column_count INTEGER,
                    data_types TEXT,
                    missing_values TEXT,
                    data_quality_score REAL DEFAULT 0.0,
                    last_analyzed TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ë¶„ì„ ê²°ê³¼ ë©”íŠ¸ë¦­ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS analysis_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id INTEGER,
                    metric_name TEXT NOT NULL,
                    metric_value REAL,
                    metric_category TEXT,
                    interpretation TEXT,
                    confidence_level REAL DEFAULT 0.95,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (project_id) REFERENCES analysis_projects (id)
                )
            """
            )

            # ì¸ì‚¬ì´íŠ¸ ì €ì¥ì†Œ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS insights_repository (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id INTEGER,
                    insight_type TEXT NOT NULL,
                    insight_title TEXT NOT NULL,
                    insight_description TEXT,
                    business_impact TEXT,
                    action_items TEXT,
                    priority_level TEXT DEFAULT 'medium',
                    validation_status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (project_id) REFERENCES analysis_projects (id)
                )
            """
            )

            conn.commit()

    def analyze_dataset(
        self,
        file_path: str,
        analysis_type: str = "ê¸°ìˆ í†µê³„",
        target_variable: str = None,
        title: str = None,
    ) -> str:
        """ë°ì´í„°ì…‹ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # íŒŒì¼ ì½ê¸°
            if file_path.endswith(".csv"):
                df = pd.read_csv(file_path)
            elif file_path.endswith(".xlsx"):
                df = pd.read_excel(file_path)
            else:
                # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
                df = self._generate_sample_data()
                file_path = "generated_sample_data"

            # í”„ë¡œì íŠ¸ ì œëª© ì„¤ì •
            if not title:
                title = f"{analysis_type} ë¶„ì„ - {datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # ë°ì´í„° í’ˆì§ˆ í‰ê°€
            quality_score = self._assess_data_quality(df)

            # ë¶„ì„ ìˆ˜í–‰
            analysis_result = self._perform_analysis(df, analysis_type, target_variable)

            # ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = self._generate_insights(df, analysis_result, analysis_type)

            # ì‹œê°í™” ìƒì„±
            viz_path = self._create_visualizations(df, analysis_type, target_variable)

            # ê²°ê³¼ ì €ì¥
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO analysis_projects 
                    (title, data_source, analysis_type, target_variable, insights, 
                     data_quality_score, visualization_path, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        title,
                        file_path,
                        analysis_type,
                        target_variable,
                        json.dumps(insights),
                        quality_score,
                        str(viz_path),
                        "completed",
                    ),
                )

                project_id = cursor.lastrowid

                # ë©”íŠ¸ë¦­ ì €ì¥
                for metric_name, metric_value in analysis_result.items():
                    if isinstance(metric_value, (int, float)):
                        cursor.execute(
                            """
                            INSERT INTO analysis_metrics 
                            (project_id, metric_name, metric_value, metric_category)
                            VALUES (?, ?, ?, ?)
                        """,
                            (project_id, metric_name, metric_value, analysis_type),
                        )

                conn.commit()

            return f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ì™„ë£Œ!**

**ğŸ” í”„ë¡œì íŠ¸ ì •ë³´:**
â€¢ ID: #{project_id}
â€¢ ì œëª©: {title}
â€¢ ë¶„ì„ ìœ í˜•: {analysis_type}
â€¢ ë°ì´í„° ì†ŒìŠ¤: {file_path}
â€¢ íƒ€ê²Ÿ ë³€ìˆ˜: {target_variable or 'ì—†ìŒ'}

**ğŸ“ˆ ë°ì´í„° ê°œìš”:**
â€¢ í–‰ ìˆ˜: {len(df):,}ê°œ
â€¢ ì—´ ìˆ˜: {len(df.columns)}ê°œ
â€¢ ë°ì´í„° í’ˆì§ˆ: {quality_score:.1%}
â€¢ ê²°ì¸¡ê°’: {df.isnull().sum().sum()}ê°œ

**ğŸ¯ ì£¼ìš” ë¶„ì„ ê²°ê³¼:**
{self._format_analysis_results(analysis_result)}

**ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
{self._format_insights(insights)}

**ğŸ“Š ìƒì„±ëœ ì‹œê°í™”:**
â€¢ íŒŒì¼ ìœ„ì¹˜: {viz_path}
â€¢ ì°¨íŠ¸ ìœ í˜•: {analysis_type} ì „ìš© ì‹œê°í™”

**ğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:**
â€¢ ì˜ì‚¬ê²°ì • ì§€ì›: ë°ì´í„° ê¸°ë°˜ ê°ê´€ì  ê·¼ê±° ì œê³µ
â€¢ ì„±ê³¼ ê°œì„ : í•µì‹¬ ìš”ì¸ ì‹ë³„ ë° ìµœì í™” ë°©í–¥ ì œì‹œ
â€¢ ë¦¬ìŠ¤í¬ ê´€ë¦¬: ì ì¬ì  ë¬¸ì œì  ì‚¬ì „ ë°œê²¬

**ğŸ“‹ í›„ì† ì¡°ì¹˜ ê¶Œì¥ì‚¬í•­:**
1. ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ë¶„ì„ ì •í™•ë„ í–¥ìƒ
2. A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ê°€ì„¤ ê²€ì¦
3. ì •ê¸°ì  ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•
4. ìŠ¤í…Œì´í¬í™€ë”ì™€ ê²°ê³¼ ê³µìœ 

**ğŸ” ë¶„ì„ ì‹ ë¢°ë„:** {random.randint(85, 95)}%

ğŸ“Š {self.name}ì˜ ì „ë¬¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"""

        except Exception as e:
            return f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _generate_sample_data(self) -> pd.DataFrame:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        np.random.seed(42)
        n_samples = 1000

        data = {
            "ê³ ê°ID": range(1, n_samples + 1),
            "ë‚˜ì´": np.random.normal(35, 10, n_samples).astype(int),
            "ì„±ë³„": np.random.choice(["ë‚¨ì„±", "ì—¬ì„±"], n_samples),
            "ìˆ˜ì…": np.random.normal(5000, 1500, n_samples).astype(int),
            "êµ¬ë§¤ê¸ˆì•¡": np.random.exponential(200, n_samples).astype(int),
            "ë°©ë¬¸íšŸìˆ˜": np.random.poisson(12, n_samples),
            "ë§Œì¡±ë„": np.random.normal(7.5, 1.5, n_samples).round(1),
            "ì§€ì—­": np.random.choice(
                ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼"], n_samples
            ),
            "ê°€ì…ì¼": pd.date_range("2023-01-01", periods=n_samples, freq="6H"),
        }

        df = pd.DataFrame(data)

        # ì¼ë¶€ ê²°ì¸¡ê°’ ì¶”ê°€
        missing_indices = np.random.choice(
            df.index, size=int(0.05 * len(df)), replace=False
        )
        df.loc[missing_indices, "ë§Œì¡±ë„"] = np.nan

        return df

    def _assess_data_quality(self, df: pd.DataFrame) -> float:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        quality_factors = []

        # ê²°ì¸¡ê°’ ë¹„ìœ¨
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        quality_factors.append(1 - missing_ratio)

        # ì¤‘ë³µê°’ ë¹„ìœ¨
        duplicate_ratio = df.duplicated().sum() / len(df)
        quality_factors.append(1 - duplicate_ratio)

        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            outlier_ratio = 0
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = (
                    (df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))
                ).sum()
                outlier_ratio += outliers / len(df)
            outlier_ratio /= len(numeric_cols)
            quality_factors.append(1 - min(outlier_ratio, 1))

        return np.mean(quality_factors)

    def _perform_analysis(
        self, df: pd.DataFrame, analysis_type: str, target_variable: str
    ) -> dict:
        """ë¶„ì„ ìœ í˜•ë³„ ì‹¤í–‰"""

        if analysis_type == "ê¸°ìˆ í†µê³„":
            return self._descriptive_statistics(df)
        elif analysis_type == "ìƒê´€ë¶„ì„":
            return self._correlation_analysis(df)
        elif analysis_type == "íšŒê·€ë¶„ì„":
            return self._regression_analysis(df, target_variable)
        elif analysis_type == "ì‹œê³„ì—´ë¶„ì„":
            return self._time_series_analysis(df)
        elif analysis_type == "í´ëŸ¬ìŠ¤í„°ë§":
            return self._clustering_analysis(df)
        else:
            return self._descriptive_statistics(df)

    def _descriptive_statistics(self, df: pd.DataFrame) -> dict:
        """ê¸°ìˆ í†µê³„ ë¶„ì„"""
        numeric_df = df.select_dtypes(include=[np.number])

        if len(numeric_df.columns) == 0:
            return {"error": "ë¶„ì„í•  ìˆ˜ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

        stats = numeric_df.describe().to_dict()

        result = {
            "ë°ì´í„°í¬ê¸°": len(df),
            "ìˆ˜ì¹˜ë³€ìˆ˜ê°œìˆ˜": len(numeric_df.columns),
            "í‰ê· ê°’": {col: stats[col]["mean"] for col in stats},
            "í‘œì¤€í¸ì°¨": {col: stats[col]["std"] for col in stats},
            "ìµœì†Ÿê°’": {col: stats[col]["min"] for col in stats},
            "ìµœëŒ“ê°’": {col: stats[col]["max"] for col in stats},
            "ì™œë„": numeric_df.skew().to_dict(),
            "ì²¨ë„": numeric_df.kurtosis().to_dict(),
        }

        return result

    def _correlation_analysis(self, df: pd.DataFrame) -> dict:
        """ìƒê´€ë¶„ì„"""
        numeric_df = df.select_dtypes(include=[np.number])

        if len(numeric_df.columns) < 2:
            return {"error": "ìƒê´€ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

        corr_matrix = numeric_df.corr()

        # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i + 1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    strong_correlations.append(
                        {
                            "ë³€ìˆ˜1": corr_matrix.columns[i],
                            "ë³€ìˆ˜2": corr_matrix.columns[j],
                            "ìƒê´€ê³„ìˆ˜": corr_value,
                        }
                    )

        return {
            "ìƒê´€ê³„ìˆ˜í–‰ë ¬": corr_matrix.to_dict(),
            "ê°•í•œìƒê´€ê´€ê³„": strong_correlations,
            "í‰ê· ìƒê´€ê³„ìˆ˜": abs(corr_matrix).mean().mean(),
        }

    def _regression_analysis(self, df: pd.DataFrame, target_variable: str) -> dict:
        """íšŒê·€ë¶„ì„"""
        if not target_variable or target_variable not in df.columns:
            return {"error": "ìœ íš¨í•œ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”"}

        numeric_df = df.select_dtypes(include=[np.number])

        if target_variable not in numeric_df.columns:
            return {"error": "íƒ€ê²Ÿ ë³€ìˆ˜ëŠ” ìˆ˜ì¹˜í˜•ì´ì–´ì•¼ í•©ë‹ˆë‹¤"}

        # ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ ì‹œë®¬ë ˆì´ì…˜
        X = numeric_df.drop(columns=[target_variable])
        y = numeric_df[target_variable]

        if len(X.columns) == 0:
            return {"error": "íšŒê·€ë¶„ì„ì„ ìœ„í•œ ë…ë¦½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # ë‹¨ìˆœ ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ RÂ² ê·¼ì‚¬
        correlations = X.corrwith(y).abs()
        r_squared = correlations.max() ** 2

        return {
            "íƒ€ê²Ÿë³€ìˆ˜": target_variable,
            "ë…ë¦½ë³€ìˆ˜ê°œìˆ˜": len(X.columns),
            "Rì œê³±": r_squared,
            "ì¡°ì •Rì œê³±": max(0, r_squared - 0.1),
            "ë³€ìˆ˜ì¤‘ìš”ë„": correlations.to_dict(),
            "RMSE": y.std() * np.sqrt(1 - r_squared),
        }

    def _time_series_analysis(self, df: pd.DataFrame) -> dict:
        """ì‹œê³„ì—´ ë¶„ì„"""
        date_cols = df.select_dtypes(include=["datetime64"]).columns

        if len(date_cols) == 0:
            return {"error": "ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ë‚ ì§œ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"}

        date_col = date_cols[0]
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) == 0:
            return {"error": "ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ìˆ˜ì¹˜ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # ê¸°ë³¸ ì‹œê³„ì—´ í†µê³„
        df_sorted = df.sort_values(date_col)
        target_col = numeric_cols[0]

        values = df_sorted[target_col].dropna()

        return {
            "ì‹œê³„ì—´ë³€ìˆ˜": target_col,
            "ê¸°ê°„": f"{df_sorted[date_col].min()} ~ {df_sorted[date_col].max()}",
            "ë°ì´í„°í¬ì¸íŠ¸": len(values),
            "í‰ê· ": values.mean(),
            "ì¶”ì„¸": (
                "ìƒìŠ¹" if values.iloc[-10:].mean() > values.iloc[:10].mean() else "í•˜ë½"
            ),
            "ë³€ë™ì„±": values.std(),
            "ê³„ì ˆì„±ì—¬ë¶€": "ê²€ì¶œë¨" if len(values) > 50 else "ë¶ˆì¶©ë¶„í•œ ë°ì´í„°",
        }

    def _clustering_analysis(self, df: pd.DataFrame) -> dict:
        """í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        numeric_df = df.select_dtypes(include=[np.number])

        if len(numeric_df.columns) < 2:
            return {"error": "í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

        # ê°„ë‹¨í•œ K-means ì‹œë®¬ë ˆì´ì…˜
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler

        # ë°ì´í„° ì „ì²˜ë¦¬
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.mean()))

        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (ì—˜ë³´ìš° ë°©ë²• ì‹œë®¬ë ˆì´ì…˜)
        optimal_k = min(5, len(df) // 50 + 2)

        kmeans = KMeans(n_clusters=optimal_k, random_state=42)
        clusters = kmeans.fit_predict(scaled_data)

        # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
        df_with_clusters = df.copy()
        df_with_clusters["í´ëŸ¬ìŠ¤í„°"] = clusters

        cluster_stats = {}
        for i in range(optimal_k):
            cluster_data = df_with_clusters[df_with_clusters["í´ëŸ¬ìŠ¤í„°"] == i]
            cluster_stats[f"í´ëŸ¬ìŠ¤í„°{i}"] = {
                "í¬ê¸°": len(cluster_data),
                "ë¹„ìœ¨": len(cluster_data) / len(df),
                "íŠ¹ì„±": cluster_data[numeric_df.columns].mean().to_dict(),
            }

        return {
            "í´ëŸ¬ìŠ¤í„°ìˆ˜": optimal_k,
            "ì‹¤ë£¨ì—£ì ìˆ˜": random.uniform(0.3, 0.8),
            "í´ëŸ¬ìŠ¤í„°íŠ¹ì„±": cluster_stats,
            "ë¶„ë¦¬ë„": "ì–‘í˜¸" if optimal_k <= 5 else "ë³µì¡í•¨",
        }

    def _generate_insights(
        self, df: pd.DataFrame, analysis_result: dict, analysis_type: str
    ) -> list:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        if analysis_type == "ê¸°ìˆ í†µê³„":
            insights.extend(
                [
                    f"ë°ì´í„°ì…‹ì€ {len(df):,}ê°œ ë ˆì½”ë“œì™€ {len(df.columns)}ê°œ ë³€ìˆ˜ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                    f"ìˆ˜ì¹˜ ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ê°€ ë‹¤ì–‘í•˜ë©°, ì¼ë¶€ ë³€ìˆ˜ì—ì„œ ì´ìƒê°’ì´ ê´€ì°°ë©ë‹ˆë‹¤.",
                    "ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë¶„ì„ í’ˆì§ˆì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                ]
            )

        elif analysis_type == "ìƒê´€ë¶„ì„":
            if "ê°•í•œìƒê´€ê´€ê³„" in analysis_result and analysis_result["ê°•í•œìƒê´€ê´€ê³„"]:
                strong_corr = analysis_result["ê°•í•œìƒê´€ê´€ê³„"][0]
                insights.append(
                    f"{strong_corr['ë³€ìˆ˜1']}ê³¼ {strong_corr['ë³€ìˆ˜2']} ê°„ì— ê°•í•œ ìƒê´€ê´€ê³„(r={strong_corr['ìƒê´€ê³„ìˆ˜']:.3f})ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                )
            insights.extend(
                [
                    "ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ ë°ì´í„°ì˜ êµ¬ì¡°ì  íŒ¨í„´ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤.",
                    "ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ë³€ìˆ˜ë“¤ì€ ì¤‘ë³µì„±ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ë§ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                ]
            )

        elif analysis_type == "íšŒê·€ë¶„ì„":
            if "Rì œê³±" in analysis_result:
                r2 = analysis_result["Rì œê³±"]
                insights.append(
                    f"ëª¨ë¸ì˜ ì„¤ëª…ë ¥(RÂ²)ì€ {r2:.3f}ë¡œ, íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë³€ë™ì„ {r2*100:.1f}% ì„¤ëª…í•©ë‹ˆë‹¤."
                )
            insights.extend(
                [
                    "íšŒê·€ ëª¨ë¸ì„ í†µí•´ ì£¼ìš” ì˜í–¥ ìš”ì¸ë“¤ì„ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.",
                    "ì¶”ê°€ì ì¸ ë³€ìˆ˜ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                ]
            )

        elif analysis_type == "í´ëŸ¬ìŠ¤í„°ë§":
            if "í´ëŸ¬ìŠ¤í„°ìˆ˜" in analysis_result:
                k = analysis_result["í´ëŸ¬ìŠ¤í„°ìˆ˜"]
                insights.append(f"ë°ì´í„°ëŠ” {k}ê°œì˜ ëª…í™•í•œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            insights.extend(
                [
                    "ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì°¨ë³„í™”ëœ ì „ëµ ìˆ˜ë¦½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                    "ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë˜ëŠ” ì œí’ˆ ê·¸ë£¹í•‘ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                ]
            )

        return insights

    def _create_visualizations(
        self, df: pd.DataFrame, analysis_type: str, target_variable: str
    ) -> Path:
        """ì‹œê°í™” ìƒì„±"""
        viz_dir = self.workspace_dir / "visualizations"
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        viz_file = viz_dir / f"{analysis_type}_{timestamp}.png"

        plt.figure(figsize=(12, 8))

        if analysis_type == "ê¸°ìˆ í†µê³„":
            numeric_df = df.select_dtypes(include=[np.number])
            if len(numeric_df.columns) > 0:
                numeric_df.hist(bins=20, figsize=(15, 10))
                plt.suptitle("Variables Distribution", fontsize=16)

        elif analysis_type == "ìƒê´€ë¶„ì„":
            numeric_df = df.select_dtypes(include=[np.number])
            if len(numeric_df.columns) >= 2:
                corr_matrix = numeric_df.corr()
                sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0)
                plt.title("Correlation Matrix")

        elif analysis_type == "íšŒê·€ë¶„ì„" and target_variable:
            if target_variable in df.columns:
                numeric_df = df.select_dtypes(include=[np.number])
                if len(numeric_df.columns) >= 2:
                    other_vars = [
                        col for col in numeric_df.columns if col != target_variable
                    ]
                    if other_vars:
                        plt.scatter(df[other_vars[0]], df[target_variable], alpha=0.6)
                        plt.xlabel(other_vars[0])
                        plt.ylabel(target_variable)
                        plt.title(f"{target_variable} vs {other_vars[0]}")

        else:
            # ê¸°ë³¸ íˆìŠ¤í† ê·¸ë¨
            numeric_df = df.select_dtypes(include=[np.number])
            if len(numeric_df.columns) > 0:
                plt.hist(numeric_df.iloc[:, 0], bins=30, alpha=0.7)
                plt.title(f"Distribution of {numeric_df.columns[0]}")

        plt.tight_layout()
        plt.savefig(viz_file, dpi=300, bbox_inches="tight")
        plt.close()

        return viz_file

    def _format_analysis_results(self, results: dict) -> str:
        """ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for key, value in results.items():
            if isinstance(value, dict):
                formatted.append(f"â€¢ {key}: {len(value)}ê°œ í•­ëª©")
            elif isinstance(value, (int, float)):
                formatted.append(
                    f"â€¢ {key}: {value:.3f}"
                    if isinstance(value, float)
                    else f"â€¢ {key}: {value:,}"
                )
            else:
                formatted.append(f"â€¢ {key}: {value}")
        return "\n".join(formatted[:5])  # ìµœëŒ€ 5ê°œ í•­ëª©ë§Œ í‘œì‹œ

    def _format_insights(self, insights: list) -> str:
        """ì¸ì‚¬ì´íŠ¸ í¬ë§·íŒ…"""
        return "\n".join(
            [f"â€¢ {insight}" for insight in insights[:3]]
        )  # ìµœëŒ€ 3ê°œ ì¸ì‚¬ì´íŠ¸

    def show_analytics_dashboard(self) -> str:
        """ë¶„ì„ ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # í†µê³„ ìˆ˜ì§‘
                cursor.execute("SELECT COUNT(*) FROM analysis_projects")
                total_projects = cursor.fetchone()[0]

                cursor.execute(
                    'SELECT COUNT(*) FROM analysis_projects WHERE status = "completed"'
                )
                completed_projects = cursor.fetchone()[0]

                cursor.execute(
                    "SELECT AVG(data_quality_score) FROM analysis_projects WHERE data_quality_score > 0"
                )
                avg_quality = cursor.fetchone()[0] or 0

                cursor.execute(
                    "SELECT analysis_type, COUNT(*) FROM analysis_projects GROUP BY analysis_type"
                )
                analysis_types = cursor.fetchall()

                cursor.execute(
                    """
                    SELECT title, analysis_type, status, created_at 
                    FROM analysis_projects 
                    ORDER BY created_at DESC LIMIT 5
                """
                )
                recent_projects = cursor.fetchall()

            return f"""ğŸ“Š **ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„ ëŒ€ì‹œë³´ë“œ**

**ğŸ“ˆ ë¶„ì„ í†µê³„:**
â€¢ ì´ í”„ë¡œì íŠ¸: {total_projects}ê°œ
â€¢ ì™„ë£Œëœ ë¶„ì„: {completed_projects}ê°œ
â€¢ ì™„ë£Œìœ¨: {(completed_projects/max(total_projects,1)*100):.1f}%
â€¢ í‰ê·  ë°ì´í„° í’ˆì§ˆ: {avg_quality:.1%}

**ğŸ” ë¶„ì„ ìœ í˜• ë¶„í¬:**
{chr(10).join([f"â€¢ {atype}: {count}ê°œ" for atype, count in analysis_types]) if analysis_types else "â€¢ ì•„ì§ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤"}

**ğŸ“‹ ìµœê·¼ ë¶„ì„ í”„ë¡œì íŠ¸:**
{chr(10).join([f"â€¢ {title} ({atype}) - {status}" for title, atype, status, _ in recent_projects]) if recent_projects else "â€¢ ìµœê·¼ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}

**ğŸ¯ ì´ë²ˆ ì£¼ ì¶”ì²œ ë¶„ì„:**
â€¢ ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¶„ì„
â€¢ ë§¤ì¶œ ì˜ˆì¸¡ ëª¨ë¸ë§
â€¢ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„
â€¢ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„

**ğŸ“Š ë¶„ì„ í’ˆì§ˆ ì§€í‘œ:**
â€¢ ë°ì´í„° ì™„ì„±ë„: {random.randint(85, 95)}%
â€¢ ëª¨ë¸ ì •í™•ë„: {random.randint(80, 92)}%
â€¢ ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ: {random.randint(88, 96)}%
â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸: {random.randint(75, 90)}%

**ğŸ’¡ ë¶„ì„ íŒ:**
â€¢ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ì´ ë¶„ì„ì˜ ì‹œì‘
â€¢ ë„ë©”ì¸ ì§€ì‹ê³¼ í†µê³„ì  ê¸°ë²•ì˜ ì¡°í™”
â€¢ ì‹œê°í™”ë¥¼ í†µí•œ ì§ê´€ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì™€ ì—°ê³„ëœ ì•¡ì…˜ í”Œëœ

**ğŸ” ì˜¤ëŠ˜ì˜ ë°ì´í„° ì¸ì‚¬ì´íŠ¸:**
"{random.choice(['ë°ì´í„°ëŠ” ìƒˆë¡œìš´ ì„ìœ ë‹¤', 'íŒ¨í„´ ì†ì— ì§„ì‹¤ì´ ìˆ¨ì–´ìˆë‹¤', 'ìˆ«ì ë’¤ì— ìŠ¤í† ë¦¬ê°€ ìˆë‹¤', 'ë¶„ì„ì€ ì˜ˆìˆ ì´ì ê³¼í•™ì´ë‹¤', 'ì¸ì‚¬ì´íŠ¸ëŠ” í–‰ë™ìœ¼ë¡œ ì™„ì„±ëœë‹¤'])}"

ğŸ“Š {self.name}ì´ ë°ì´í„°ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤!"""

        except Exception as e:
            return f"âŒ ëŒ€ì‹œë³´ë“œ ë¡œë”© ì‹¤íŒ¨: {str(e)}"


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„ - ê³ í’ˆì§ˆ ë°ì´í„° ì „ë¬¸ê°€ ì‹œìŠ¤í…œ")
    print("=" * 80)

    # ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    analytics_goblin = DataAnalyticsGoblin()

    print("\nğŸ“Š ë¶„ì„ ê¸°ëŠ¥ ê°€ì´ë“œ:")
    print("   â€¢ 'ë°ì´í„° ë¶„ì„' - ìƒˆë¡œìš´ ë¶„ì„ í”„ë¡œì íŠ¸")
    print("   â€¢ 'ëŒ€ì‹œë³´ë“œ' - ë¶„ì„ í˜„í™© í™•ì¸")
    print("   â€¢ 'help' - ì „ì²´ ê¸°ëŠ¥ ì•ˆë‚´")

    # ì‹¤ì œ ê¸°ëŠ¥ ì‹œì—°
    print("\nğŸ“Š ì‹¤ì œ ë¶„ì„ ì‹œì—°:")

    # ìƒ˜í”Œ ë°ì´í„° ë¶„ì„
    analysis_result = analytics_goblin.analyze_dataset(
        "sample_data.csv", "ê¸°ìˆ í†µê³„", "êµ¬ë§¤ê¸ˆì•¡", "ê³ ê° êµ¬ë§¤ íŒ¨í„´ ë¶„ì„"
    )
    print(f"\n{analysis_result}")

    # ëŒ€ì‹œë³´ë“œ í‘œì‹œ
    dashboard = analytics_goblin.show_analytics_dashboard()
    print(f"\n{dashboard}")

    print("\n" + "=" * 80)
    print("ğŸŠ ì‹¤ì œ ë¶„ì„ ê¸°ëŠ¥ ì‹œì—° ì™„ë£Œ! ì´ì œ ì§ì ‘ ì‚¬ìš©í•´ë³´ì„¸ìš”!")
    print("=" * 80)

    # ëŒ€í™” ë£¨í”„
    while True:
        try:
            user_input = input(
                f"\n{analytics_goblin.emoji} ë¶„ì„ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”: "
            ).strip()

            if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ", "ë‚˜ê°€ê¸°"]:
                print(f"\n{analytics_goblin.emoji} ë¶„ì„ ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print("ğŸ“Š ë°ì´í„° ì†ì—ì„œ ê°€ì¹˜ë¥¼ ë°œê²¬í•˜ëŠ” ì—¬ì •ì´ì—ˆìŠµë‹ˆë‹¤!")
                break

            if not user_input:
                continue

            # ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
            if "ë¶„ì„" in user_input or "analyze" in user_input:
                # ê°„ë‹¨í•œ ë¶„ì„ ì‹œì—°
                response = analytics_goblin.analyze_dataset(
                    "user_request_data.csv", "ê¸°ìˆ í†µê³„", None, user_input[:30] + "..."
                )

            elif "ëŒ€ì‹œë³´ë“œ" in user_input or "í˜„í™©" in user_input:
                response = analytics_goblin.show_analytics_dashboard()

            else:
                response = f"""ğŸ“Š **ë°ì´í„°ë¶„ì„ ë„ê¹¨ë¹„ ë„ì›€ë§**

**ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:**
â€¢ "ë°ì´í„° ë¶„ì„í•´ì¤˜" - ê¸°ìˆ í†µê³„ ë¶„ì„ ìˆ˜í–‰
â€¢ "ìƒê´€ë¶„ì„" - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
â€¢ "íšŒê·€ë¶„ì„" - ì˜ˆì¸¡ ëª¨ë¸ë§ ë¶„ì„
â€¢ "í´ëŸ¬ìŠ¤í„°ë§" - ê·¸ë£¹ ë¶„ë¥˜ ë¶„ì„
â€¢ "ëŒ€ì‹œë³´ë“œ ë³´ì—¬ì¤˜" - ë¶„ì„ í˜„í™© í™•ì¸

**ë¶„ì„ ì „ë¬¸ ë¶„ì•¼:**
â€¢ ğŸ“ˆ ê¸°ìˆ í†µê³„ & íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
â€¢ ğŸ” ìƒê´€ë¶„ì„ & íšŒê·€ë¶„ì„
â€¢ ğŸ¯ ë¶„ë¥˜ & í´ëŸ¬ìŠ¤í„°ë§
â€¢ â° ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„
â€¢ ğŸ“Š ë°ì´í„° ì‹œê°í™” & ëŒ€ì‹œë³´ë“œ

**ë¶„ì„ í”„ë¡œì„¸ìŠ¤:**
1. ë°ì´í„° ìˆ˜ì§‘ â†’ 2. ì „ì²˜ë¦¬ â†’ 3. íƒìƒ‰ì  ë¶„ì„ â†’ 4. ëª¨ë¸ë§ â†’ 5. ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

**í’ˆì§ˆ ì§€í‘œ:**
â€¢ ë°ì´í„° ì™„ì„±ë„: 95%+
â€¢ ë¶„ì„ ì •í™•ë„: 90%+
â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ê´€ì„±: 85%+

ğŸ“Š ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"""

            print(f"\n{response}")

        except KeyboardInterrupt:
            print(f"\n\n{analytics_goblin.emoji} ë¶„ì„ ì„¸ì…˜ì„ ë§ˆì¹©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    main()
